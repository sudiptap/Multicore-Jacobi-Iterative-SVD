\documentclass[10pt, conference, compsocconf]{IEEEtran}

\usepackage{cite}
\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{algorithmic}
\usepackage{array}

\usepackage{mdwmath}
\usepackage{mdwtab}


\usepackage{eqparbox}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{fixltx2e}
\usepackage{stfloats}
\usepackage{url}
\usepackage{amssymb}
\usepackage{filecontents}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{On Speeding-up Parallel Jacobi Iterations for SVDs}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Soumitra Pal \qquad Sudipta Pathak \qquad Sanguthevar Rajasekaran}
\IEEEauthorblockA{Computer Science and Engineering, University of Connecticut\\
371 Fairfield Road, Storrs, CT 06269, USA\\
\texttt{\small \em \{mitra@,sudipta.pathak@,rajasek@engr.\}uconn.edu}}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}



% make the title area
\maketitle


\begin{abstract}
We live in an era of big data and the analysis of these data is becoming a bottleneck in many domains including biology and the internet. To make these analyses feasible in practice, we need efficient data reduction algorithms. The Singular Value Decomposition (SVD) is a data reduction technique that has been used in many different applications. For example, SVDs have been extensively used in text analysis. Several sequential algorithms have been developed for the computation of SVDs. The best known sequential algorithms take cubic time which may not be acceptable in practice. As a result, many parallel algorithms have been proposed in the literature. There are two kinds of algorithms for SVD, namely, QR decomposition and Jacobi iterations. Researchers have found out that even though QR is sequentially faster than Jacobi iterations, QR is difficult to parallelize. As a result, most of the parallel algorithms in the literature are based on Jacobi iterations. JRS is an algorithm that has been shown to be very effective in parallel. JRS is a relaxation of the classical Jacobi algorithm. In this paper we propose a novel variant of the classical Jacobi algorithm that is more efficient than the JRS algorithm. Our experimental results confirm this assertion. We also provide a convergence proof for our new algorithm. We show how to efficiently implement our algorithm on such parallel models as the PRAM and the mesh. 
\end{abstract}

\begin{IEEEkeywords}
SVD; Jacobi iterations; JRS; parallel algorithms
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
Singular Value Decomposition (SVD) is a fundamental computational problem in linear algebra and it has application in various computational science and engineering areas. For example, it is widely used in areas such as statistics where it is directly related to principal component analysis, in signal processing and pattern recognition as an essential filtering tool, and in control systems. Recently, it is used as one of the fundamental steps in many machine learning applications such as least square regressions, information retrieval and so on. With the advent of BigData, it has become essential to process data matrices with thousands of rows and columns in real time. The SVD is one of the data reduction techniques. Hence, there is a strong need for efficient sequential and parallel algorithms for the SVD.

SVD takes as input a matrix $A \in \mathbb{F}^{m \times n}$ where $\mathbb{F}$ could be the field of real ($\mathbb{R}$) or complex ($\mathbb{C}$) numbers and outputs three matrices $U, S, V$ such that $A = USV^T$, where $U \in \mathbb{F}^{m \times m}$, $V \in \mathbb{F}^{n \times n}$ are orthogonal matrices (i.e. $U^TU = I_m$, $V^TV = I_n$) and $S \in \mathbb{R}^{m \times n}$ is a diagonal matrix. If $S = diag(\sigma_1, \sigma_2,......\sigma_{\min\{m,n\}})$, then the diagonal elements $\sigma_i$s are called the singular values of $A$. The columns of $U$ and $V$ are referred to as the left and right singular vectors respectively. Without loss of generality, we assume that $m \ge n$. 

There are various methods of computing the SVD~\cite{golub2012matrix}. The most commonly used algorithms for dense matrices, which we consider in this paper, can be classified as \emph{QR-based} and \emph{Jacobi-based}. The QR-based algorithms work in two stages. In the first stage the input matrix is converted to a band matrix (bidiagonal, tridiagonal and so on) using factorizations such as Cholesky, LU and QR. In the final stage the band matrix is converted to diagonal form 


\subsection{Two-sided Jacobi Algorithm}

\par Among all algorithms for finding SVD Jacobi iteration based algorithms are most widely used in parallel settings since they are easier to parallelize than others. There are two types of Jacobi algorithms for SVD, namely Two-sided Jacobi SVD algorithm and One-sided Jacobi SVD algorithm. Two-sided Jacobi SVD algorithm is applicable to symmetric matrices only. It applies a series of \textit{Jacobi Rotations} on input matrix $A$ to diagonalize it. 
\[
A_{k+1} = U_kA_kU_k^T, k =1,2,......
\]   
where $U_k = U_k(i,j,\phi^k_{ij})$ represents a rotation of $(i,j)$-plane. 
\begin{gather}
u_{ii}^k = u_{jj}^k = c^k = \text{cos}(\phi^k_{ij}) \\
u_{ij}^k = -u_{ji}^k = s^k = \text{sin}(\phi^k_{ij}) 
\end{gather}

$\phi^k$ is chosen such that 
\begin{gather}
b^{k+1}_{ij} = b^{k+1}_{ji} = 0,\\
\text{or} \thinspace \text{tan}(2\phi^k_{ij}) = \frac{2b^k_{ij}}{b^k_{ii} - b^k_{jj}} \\
\text{and} \thinspace\text{where} |\phi^k_{ij}| \leq \frac{1}{4}\pi 
\end{gather}
$c^k$ is computed by 
\begin{gather}
c^k = \frac{1}{\sqrt{1+t^2_k}} \thinspace \text{and} s^k = c^kt^k \\
\text{where} t^k \text{is the smaller root (in magnitude) of the quadratic equation given by} \\
{t^k}^2 + 2\alpha^kt^k - 1 = 0, \alpha^k = \text{cot}(2\phi^k_{ij}) \\
\text{hence}, t^k = \frac{sign(\alpha^k)}{|\alpha^k| + \sqrt{1+{\alpha^k}^2}}
\end{gather}
$B_{k+1}$ is identical to $B_k$ other than the rows and columns $i$ and $j$ and the modified elements are given by 
\begin{gather}
b_{ii}^{k+1} = b_{ii}^k + t^{k}b_{ij}^k \\
b_{jj}^{k+1} = b_jj^k - t^kb_{ij}^k \\
b_{ix}^{k+1} = c^{k}b_{ix}^k + s^{k}_{jr} \\
b_{jx}^{k+1} = -s_kb^k_{ix} + c_kb^k_{jx} \\
x \neq i,j
\end{gather}
The above algorithm ensures that with every rotation $A_k$ approaches the diagonal matrix $S$ = diag$(\sigma_1, \sigma_2,....\sigma_n)$ and $(U_k......U_2U_1)^T$ approaches a matrix whose $j-$th column is the eigenvector corresponding to $\sigma_j$. 
\cite{luk1989proof} shows that each sweepcauses the off-diagonal norm to decrase and evetually the algorithm converges. According to the original Jacobi scheme one should search for the largest off-diagonal element to eliminate at every rotation. However, this scheme is extremely time consuming in practice. Hence, a common scheme used is to choose elements $(i,j)$ to be annihilated in cyclic order $(1,2), (1,3), (1,4),.....(1,n),(2,3),......(2,n),......,(n-1,n)$. This is known as cyclic jacobi algorithm. \cite{forsythe1960cyclic, schonhage1961konvergenz, wilkinson1962note} show that cyclic Jacobi algorithm has a quadratic convergence rate and argues that convergence ususally occurs within 6 to 10 sweeps or $3n^2$ to $5n^2$ Jacobi rotations.

\subsection{One-sided Jacobi Algorithm}

\par A more general and more widely used algorithm is One-Sided Jacobi SVD algorithm that deals with any matrix. One-Side Jacobi SVD algorithm finds an orthogonal matrix $V \in \mathbb{R}^{n \times n}$, such that $B = (b_1, b_2,.......,b_n) = AV$ where $B \in \mathbb{R}^{m \times n}$ is an orthogonal matrix. The above statement implies that:
\[
b_i^Tb_j = \sigma_i^2\delta_{ij}
\]
where
\[
\delta_{ij}= 
\begin{cases}
    0,& \text{for }i\neq j\\
    1,& \text{for }i = j\\
\end{cases}
\]

The quantities $\sigma_i$ are called \textit{Singular Values} of matrix $A$. $B$ can be written as $B = US$ where $U = (u_1, u_2,....,u_k)$such that $u_j = \frac{b_j}{\sigma_j}$ for $\sigma_j \neq 0$ and $U^TU = I \in \mathbb{R}^{n \times n}$. Hence we can now write $B = AV$ as $US = AV$ or using properties of orthogonality $A = USV^T$. If some of the $\sigma_j$s are zeros singular values, the corresponding columns of $U$ are set to null vectors. The first $k$ singular values are always chosen to be non-zero causing the matrix $U^TU$ to be unit matrix of order $k$ augmented to order n with zeros. This is written as follows :

\begin{equation}
S = \left( \begin{array}{cc}
\textbf{1}_k & \hfill\\
\hfill & \textbf{0}_{n-k}\\
\end{array} \right)
\end{equation} 

The matrix $V$ used for orthogonalization of $A$ is a product of matrices indexed by $V^{(k)}$.
\[
V = \prod_{k=1}^{z}V^{(k)}
\]

$V$ is constructed in such a way that
\begin{equation}
(a_i, a_j) \left[ \begin{array}{cc}
c & -s \\
s & c \\
\end{array} \right]
= (a_i^{k+1}, a_j^{k+1}) 
\end{equation}
, $i < j$ and $||a_i^{k+1}||_2 > ||a_j^{k+1}||_2$,
here $a_i$ is the $i-$th column of $A$. This is achieved by choosing 

$c = [\frac{\beta + \gamma}{2\gamma}]^{\frac{1}{2}}$ and $s = [\frac{\alpha}{2\gamma c}]$,
$s = [\frac{\gamma - \beta}{2\gamma}]^{\frac{1}{2}}$ and $c = [\frac{\alpha}{2\gamma s}]$

where $\alpha = 2a_i^Ta_j$, $\beta = ||a_j||^2_2$ and $\gamma = (\alpha^2 + \beta^2)^{\frac{1}{2}}$

The matrices given by $V^{(k)}$ are called rotation matrices and $z$ is not dependent of dimension of $A$.

Describe one sided Jacobi with math notations from Dr Rajs paper:
\subsection{Block Householder Jacobi}
For $A \in \mathbb{R}^{m \times n}$ where $m >> n$, a common practice is to reduce the problem complexity by applying initial orthogonal factorization on $A$ followed by one-sided Jacobi method. Given $A$ we apply the following orthogonal facotirzation given by $A = QR$, where $Q \in \mathbb{R}^{m \times n}$ is an orthogonal matrix and $R \in \mathbb{R}^{n \times n}$ is an upper triangular matrix. Next, One-sided Jacobi SVD algorithm is applied on $R$. The Block Householder- Jacobi SVD algorithm is presented below:
Step 1: Factorize A by $A = QR$ where $Q \in \mathbb{R}^{m \times n}$ with orthonormal columns and $R \in \mathbb{R}^{n \times n}$ is upper triangular. \\
Step 2: Determine SVD of the upper triangular matrix using One-sided Jacobi algorithm.
\begin{equation}
R = \hat{U}\left[ \begin{array}{c}
\hat{S} \\
\text{0}\\
\end{array} \right]
V^T
\end{equation}
where $\hat{U} \in \mathbb{R}{n \times p}$ and $V \in \mathbb{R}{n \times p}$ orthogonal matrices and $\hat{S} =$ diag$(\sigma_i)$ is a diagonal matrix containing $p$ non-zero singular values of $A$. \\
Step 3: Compute $U = Q\hat{U}$. Left singular vectors of $A$ are given by $u_i$ where $u_i$ are columns of $U$. 

\cite{bevcka2013parallel, kudo2016parallel, becka2015parallel} introduced and implemented the idea of parallel one-sided block jacobi algorithm with dynamic ordering and variable blocking. Their approach, known as OSBJ method, accomplishes the parallel SVD in three stages namely, pre-processing, iteration and post processing. There are two types of pre-processing depending on the dimension of the matrix, namely QR-pre-processing and LQ pre-processing. During QR-pre-processing OSBJ decomposes input matrix $A = Q_1R$ where $A \in \mathbb{R}^{m \times n}$, orthonormal matrix $Q_1 \in \mathbb{R}^{m \times m}$ and upper triangular matrix $R \in \mathbb{R}^{n \times n}$. On the contrary, LQ-pre-processing decomposes input matrix $A = LQ_2$ where $L \in \mathbb{R}^{n \times n}$ is a lower triangular matrix and $Q_2 \in \mathbb{R^{n \times n}}$ is an orthogonal matrix. The $L$ or $R$ matrix in pre-processing stage is considered as input $A^0$ for the iteration stage.
$A^0$ is partitioned into column blocks. Let, $A^{(r)}_i$ and $A^{(r)}_j$ are one such column block pair where $1 \leq i < j \leq l$ assuming we have $l$ such column blocks. Weight $\hat{w}^{r}_{ij} = \frac{||{A^{(r)}_i}^TA^{(r)}_j\textbf{e}||_2}{||\textbf{e}||_2}$, where $\textbf{e} = (1, 1, ....1)^T \in \mathbb{R}^{\frac{n}{l}}$.  $\hat{w}^{r}_{ij}$ serves as an approximate measure of inclination between $Im(A^{r}_i)$ and $Im(A^{r}_j)$. The column block pairs are ordered based on their mutual inclination and the most mutually inclined pair is chosen to be orthogonalized and those columns are excluded from the set for next iteration. The process is repeated until all the column blocks are used up. For each column block pair a $2 \times 2$ Gram matrix $G_s \in \mathbb{R}^{\frac{2n}{l} \times \frac{2n}{l}}$ is constructed as is given by $G_s = [A^{(r)}_{i_{r,s}},A^{(r)}_{j_{r,s}}]^T[A^{(r)}_{i_{r,s}},A^{(r)}_{j_{r,s}}]$ where $1 \leq s \leq \frac{l}{2}$. This gram matrix is them diagonalized as $G_s = V_sD_sV_s^T$ where $V_s$ is an orthogonal matrix and $D_s$ is a diagonal matrix. The column blocks for next iteration $A^{(r+1)}$ is computed by $[A^{(r+1)}_{i_{r,s}},A^{(r+1)}_{j_{r,s}}] = [A^{(r)}_{i_{r,s}},A^{(r)}_{j_{r,s}}]V_s$. Iteration process continues until all the columns are mutually orthogonal. 
\par The Jacobi Relaxation Scheme (JRS) algorithm by Rajasekaran and Song introduced the idea of improving parallelism in SVD computation by 
multiplying the off-diagonal element in each iteration by a very small number $\epsilon$ such that $0 < \epsilon < 1$ instead of setting the off-diagonal element to zero. \cite{rajasekaran2008relaxation} compares number of iterations taken by JRS algorithm to converge with that of Strumpen's Independent Jacobi algorithm \cite{strumpen2003stream} and shows that JRS takes much less number of iterations to converge than Independent Jacobi. 

This paper has done same parallel implementation:~\cite{soliman2008memory}.

This paper seems to do some kind of sorting for~\cite{zhou1995parallel}.

\subsection{Contributions}
Subsection text here.


\section{Quick Pivoting}
Wherever Times is specified, Times Roman or Times New Roman may be used. If neither is available on your system, please use the font closest in appearance to Times. Avoid using bit-mapped fonts if possible. True-Type 1 or Open Type fonts are preferred. Please embed symbol fonts, as well, for math, etc.




\section{Parallel Implementation}
Wherever Times is specified, Times Roman or Times New Roman may be used. If neither is available on your system, please use the font closest in appearance to Times. Avoid using bit-mapped fonts if possible. True-Type 1 or Open Type fonts are preferred. Please embed symbol fonts, as well, for math, etc.



\section{Experimental Results}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.



\section{Conclusion}
The conclusion goes here. this is more of the conclusion

% conference papers do not normally have an appendix


% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank...
more thanks here


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,jacobi}


% that's all folks
\end{document}


